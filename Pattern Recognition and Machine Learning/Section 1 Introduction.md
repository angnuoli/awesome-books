<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML"></script>

# 1.1 Polynomial Curve Fitting

> The result of running the machine learning algorithm can be expressed as a function **y(x)** which takes a new digit image **x** as input and that generates an output vector y, encoded in the same way as the target vectors. 

Mapping.

> The ability to categorize correctly new examples that differ from those used for training is known as generalization. 

What we want is to predict new data based on data we have.

> Applications in which the training data comprises examples of the input vectors along with their corresponding target vectors are known as *supervised learning* problems. 

## Over-fitting Problem

Increasing size of the data set reduces the over-fitting problem.

# 1.2 Probability Theory

The Rules of Probability
1. sum rule 	  $$p(X) = \sum_{Y}p(X,Y)$$
2. product rule $$p(X,Y)=p(Y|X)p(X)$$
